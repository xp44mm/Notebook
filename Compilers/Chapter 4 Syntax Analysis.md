# Chapter 4 Syntax Analysis

This chapter is devoted to parsing methods that are typically used in compilers. We first present the basic concepts, then techniques suitable for hand implementation, and finally algorithms that have been used in automated tools. Since programs may contain syntactic errors, we discuss extensions of the parsing methods for recovery from common errors.

By design, every programming language has precise rules that prescribe the syntactic structure of well-formed programs. In C, for example, a program is made up of functions, a function out of declarations and statements, a statement out of expressions, and so on. The syntax of programming language constructs can be specified by context-free grammars or BNF (Backus-Naur Form) notation, introduced in Section 2.2. Grammars order significant benefits for both language designers and compiler writers.

- A grammar gives a precise, yet easy-to-understand, syntactic specification of a programming language.

- From certain classes of grammars, we can construct automatically an efficient parser that determines the syntactic structure of a source program. As a side benefit, the parser-construction process can reveal syntactic ambiguities and trouble spots that might have slipped through the initial design phase of a language.

- The structure imparted to a language by a properly designed grammar is useful for translating source programs into correct object code and for detecting errors.

- A grammar allows a language to be evolved or developed iteratively, by adding new constructs to perform new tasks. These new constructs can be integrated more easily into an implementation that follows the grammatical structure of the language.

## 4.2 Context-Free Grammars

Grammars were introduced in Section 2.2 to systematically describe the syntax of programming language constructs like expressions and statements. Using a syntactic variable `stmt` to denote statements and variable `expr` to denote expressions, the production

$$
stmt \to \textbf{if } ( expr )\ stmt\ \textbf{else}\ stmt \tag{4.4}
$$

specifies the structure of this form of conditional statement. Other productions then define precisely what an `expr` is and what else a `stmt` can be.

This section reviews the definition of a context-free grammar and introduces terminology for talking about parsing. In particular, the notion of derivations is very helpful for discussing the order in which productions are applied during parsing.

### 4.2.1 The Formal Definition of a Context-Free Grammar

From Section 2.2, a context-free grammar (grammar for short) consists of terminals, nonterminals, a start symbol, and productions.

1. **Terminals** are the basic symbols from which strings are formed. The term “token name” is a synonym for “terminal” and frequently we will use the word “token” for terminal when it is clear that we are talking about just the token name. We assume that the terminals are the first components of the tokens output by the lexical analyzer. In (4.4), the terminals are the keywords if and else and the symbols `(` and `)`.

2. **Nonterminals** are syntactic variables that denote sets of strings. In (4.4), `stmt` and `expr` are nonterminals. The sets of strings denoted by nonterminals help define the language generated by the grammar. Nonterminals impose a hierarchical structure on the language that is key to syntax analysis and translation.

3. In a grammar, one nonterminal is distinguished as the *start symbol*, and the set of strings it denotes is the language generated by the grammar. Conventionally, the productions for the start symbol are listed first.

4. The productions of a grammar specify the manner in which the terminals and nonterminals can be combined to form strings. Each production consists of:

   (a) A nonterminal called the *head* or *left side* of the production; this production defines some of the strings denoted by the head.

   (b) The symbol `->`. Sometimes `::=` has been used in place of the arrow.

   (c) A *body* or *right side* consisting of zero or more terminals and nonterminals. The components of the body describe one way in which strings of the nonterminal at the head can be constructed.

**Example 4.5:** The grammar in Fig. 4.2 defines simple arithmetic expressions. In this grammar, the terminal symbols are

$$
\textbf{id}+-*/()
$$

The nonterminal symbols are $expression$, $term$ and $factor$, and $expression$ is the start symbol. □

$$
\begin{array}{rrl}
expression & \to & expression + term &\\
expression & \to & expression - term &\\
expression & \to & term &	\\
term       & \to & term * factor&	 \\
term       & \to & term / factor	& \\
term       & \to & factor	& \\
factor     & \to & ( expression )	&\\
factor     & \to & \textbf{id} &
\end{array}
$$

Figure 4.2: Grammar for simple arithmetic expressions

### 4.2.2 Notational Conventions

To avoid always having to state that “these are the terminals,” “these are the nonterminals,” and so on, the following notational conventions for grammars will be used throughout the remainder of this book.

1. These symbols are terminals:

   (a) Lowercase letters early in the alphabet, such as $a, b, c$.

   (b) Operator symbols such as $+$, $ * $, and so on.

   (c) Punctuation symbols such as parentheses, comma, and so on.

   (d) The digits $0, 1, \dots , 9$.

   (e) Boldface strings such as **id** or **if**, each of which represents a single terminal symbol.

2. These symbols are nonterminals:

   (a) Uppercase letters early in the alphabet, such as $A, B, C$.

   (b) The letter $S$, which, when it appears, is usually the start symbol.

   (c) Lowercase, italic names such as *expr* or *stmt*.

   (d) When discussing programming constructs, uppercase letters may be used to represent nonterminals for the constructs. For example, nonterminals for expressions, terms, and factors are often represented by $E$, $T$, and $F$, respectively.

3. Uppercase letters late in the alphabet, such as $X$, $Y$, $Z$, represent grammar symbols; that is, either nonterminals or terminals.

4. Lowercase letters late in the alphabet, chiefly $u, v, \dots , z$, represent (possibly empty) strings of terminals.

5. Lowercase Greek letters, $\alpha$, $\beta$, $\gamma$  for example, represent (possibly empty) strings of grammar symbols. Thus, a generic production can be written as $A \to \alpha$, where $A$ is the head and $\alpha$ the body.

6. A set of productions $A \to \alpha_1$, $A \to \alpha_2$, $\dots$, $A \to \alpha_k$ with a common head $A$ (call them A-productions), may be written $A \to \alpha_1 | \alpha_2 | \dots | \alpha_k$. Call $\alpha_1, \alpha_2, \dots, \alpha_k$ the *alternatives* for $A$.

7. Unless stated otherwise, the head of the first production is the start symbol.

**Example 4.6:** Using these conventions, the grammar of Example 4.5 can be rewritten concisely as

$$
\begin{array}{ll}
E & \to & E + T | E - T | T \\
T & \to & T * F | T / F | F \\
F & \to & ( E ) | \textbf{id}
\end{array}
$$

The notational conventions tell us that $E$, $T$, and $F$ are nonterminals, with $E$ the start symbol. The remaining symbols are terminals. □

### 4.2.3 Derivations

The construction of a parse tree can be made precise by taking a derivational view, in which productions are treated as rewriting rules. Beginning with the start symbol, each rewriting step replaces a nonterminal by the body of one of its productions. This derivational view corresponds to the top-down construction of a parse tree, but the precision a order by derivations will be especially helpful when bottom-up parsing is discussed. As we shall see, bottom-up parsing is related to a class of derivations known as “rightmost” derivations, in which the rightmost nonterminal is rewritten at each step.

For example, consider the following grammar, with a single nonterminal $E$, which adds a production $E \to - E$ to the grammar (4.3):

$$
E \to E + E | E * E | - E | ( E ) |\textbf{id} \tag{4.7}
$$

The production $E \to - E$ signifies that if $E$ denotes an expression, then $- E$ must also denote an expression. The replacement of a single $E$ by $- E$ will be described by writing

$$
E \implies - E
$$

which is read, “$E$ derives $- E$.” The production $E \to ( E )$ can be applied to replace any instance of $E$ in any string of grammar symbols by $( E )$, e.g., $E * E \implies  (E ) * E$ or $E * E \implies  E * ( E )$. We can take a single $E$ and repeatedly apply productions in any order to get a sequence of replacements. For example,

$$
E \implies - E \implies - ( E ) \implies - (\textbf{id})
$$

We call such a sequence of replacements a *derivation* of $- (\textbf{id})$ from $E$. This derivation provides a proof that the string $- (\textbf{id})$ is one particular instance of an expression.

For a general definition of derivation, consider a nonterminal $A$ in the middle of a sequence of grammar symbols, as in $\alpha A\beta$, where $\alpha$ and $\beta$ are arbitrary strings of grammar symbols. Suppose $A \to \gamma$ is a production. Then, we write $\alpha A\beta\implies \alpha \gamma\beta$. The symbol $\implies$  means, “derives in one step.” When a sequence of derivation steps $\alpha_1 \implies  \alpha_2 \implies  \dots  \implies  \alpha_n$ rewrites $\alpha_1$ to $\alpha_n$, we say $\alpha_1$ derives $\alpha_n$. Often, we wish to say, “derives in zero or more steps.” For this purpose, we can use the symbol $\overset*\implies$. Thus,

1. $\alpha \overset*\implies  \alpha$, for any string $\alpha$, and

2. If $\alpha \overset*\implies  \beta$ and $\beta \implies \gamma$, then $\alpha \overset*\implies \gamma$.

Likewise, $\overset+\implies$  means, “derives in one or more steps.”

If $S \overset*\implies \alpha$, where $S$ is the start symbol of a grammar *G*, we say that $\alpha$ is a *sentential form* of *G*. Note that a sentential form may contain both terminals and nonterminals, and may be empty. A *sentence* of *G* is a sentential form with no nonterminals. The *language* generated by a grammar is its set of sentences. Thus, a string of terminals $w$  is in $L(G)$, the language generated by *G*, if and only if $w$  is a sentence of *G* (or $S \overset*\implies w$). A language that can be generated by a grammar is said to be a *context-free language*. If two grammars generate the same language, the grammars are said to be *equivalent*.

The string $- (\textbf{id} + \textbf{id})$ is a sentence of grammar (4.7) because there is a derivation

$$
E \implies - E \implies - (E) \implies - (E + E) \implies - (id + E) \implies  - (\textbf{id} + \textbf{id}) \tag{4.8}
$$

The strings $E, - E, - (E), \dots  , - (\textbf{id} + \textbf{id})$ are all sentential forms of this grammar. We write $E \overset*\implies  - (\textbf{id} + \textbf{id})$ to indicate that $- (\textbf{id} + \textbf{id})$ can be derived from $E$.

At each step in a derivation, there are two choices to be made. We need to choose which nonterminal to replace, and having made this choice, we must pick a production with that nonterminal as head. For example, the following alternative derivation of $- (\textbf{id} + \textbf{id})$ differs from derivation (4.8) in the last two steps:

$$
E \implies - E \implies - (E) \implies - (E + E) \implies - (E + \textbf{id}) \implies - (\textbf{id} + \textbf{id}) \tag{4.9}
$$

Each nonterminal is replaced by the same body in the two derivations, but the order of replacements is different.

To understand how parsers work, we shall consider derivations in which the nonterminal to be replaced at each step is chosen as follows:

1. In *leftmost* derivations, the leftmost nonterminal in each sentential is always chosen. If $\alpha\implies \beta$ is a step in which the leftmost nonterminal in $\alpha$ is replaced, we write $\alpha \underset{lm}\implies \beta$.

2. In *rightmost* derivations, the rightmost nonterminal is always chosen; we write $\alpha \underset{rm}\implies  \beta$ in this case.

Derivation (4.8) is leftmost, so it can be rewritten as

$$
E \underset{lm}\implies  - E \underset{lm}\implies  - (E) \underset{lm}\implies  - (E + E) \underset{lm}\implies  - (\textbf{id} + E) \underset{lm}\implies  - (\textbf{id} + \textbf{id})
$$

Note that (4.9) is a rightmost derivation.

Using our notational conventions, every leftmost step can be written as $w A\gamma \underset{lm}\implies  w\delta\gamma$, where $w$  consists of terminals only, $A \to \delta$ is the production applied, and $\gamma$ is a string of grammar symbols. To emphasize that $\alpha$ derives $\beta$ by a leftmost derivation, we write $\alpha \underset{lm}{\overset * \implies}  \beta$. If $S \underset{lm}{\overset * \implies} \alpha$, then we say that $\alpha$ is a *left-sentential form* of the grammar at hand.

Analogous definitions hold for rightmost derivations. Rightmost derivations are sometimes called *canonical* derivations.

### 4.2.4 Parse Trees and Derivations

A parse tree is a graphical representation of a derivation that alters out the order in which productions are applied to replace nonterminals. Each interior node of a parse tree represents the application of a production. The interior node is labeled with the nonterminal $A$ in the head of the production; the children of the node are labeled, from left to right, by the symbols in the body of the production by which this A was replaced during the derivation.

For example, the parse tree for $- (\textbf{id} + \textbf{id})$ in Fig. 4.3, results from the derivation (4.8) as well as derivation (4.9).

```
    E
-        E
    (    E    )
       E + E
      id   id
```

Figure 4.3: Parse tree for $- (\textbf{id} + \textbf{id})$

The leaves of a parse tree are labeled by nonterminals or terminals and, read from left to right, constitute a sentential form, called the *yield* or *frontier* of the tree.

節點路徑會組成一個序列，序列按F#值進行排序，小值在左邊，大值在右邊。比如：

```F#
let a = paths.['('] = [ 0; 1; 0]
let b = paths.['+'] = [ 0; 1; 1; 1]
a < b -> '(' 在 '+' 左邊
```

To see the relationship between derivations and parse trees, consider any derivation $\alpha_1 \implies  \alpha_2 \implies  \dots  \implies  \alpha_n$, where $\alpha_1$ is a single nonterminal $A$. For each sentential form $\alpha_i$ in the derivation, we can construct a parse tree whose yield is $\alpha_i$. The process is an induction on $i$.

**BASIS:** The tree for $\alpha_1 = A$ is a single node labeled $A$.

**INDUCTION:** Suppose we already have constructed a parse tree with yield $\alpha_{i-1} = X_1 X_2 \dots  X_k$ (note that according to our notational conventions, each grammar symbol $X_i$ is either a nonterminal or a terminal). Suppose $\alpha_i$ is derived from $\alpha_{i-1}$ by replacing $X_j$, a nonterminal, by $\beta = Y_1 Y_2 \dots  Y_m$. That is, at the ith step of the derivation, production $X_j \to \beta$ is applied to $\alpha_{i-1}$ to derive $\alpha_i = X_1 X_2 \dots  X _ {j-1} \beta X _ {j+1} \dots  X_k$.

To model this step of the derivation, find the $j$th non-$\epsilon$ leaf from the left in the current parse tree. This leaf is labeled $X_j$. Give this leaf $m$ children, labeled $Y_1, Y_2, \dots , Y_m$, from the left. As a special case, if $m = 0$, then $\beta = \epsilon$, and we give the $j$th leaf one child labeled $\epsilon$.

**Example 4.10:** The sequence of parse trees constructed from the derivation (4.8) is shown in Fig. 4.4. In the first step of the derivation, $E \implies  - E$. To model this step, add two children, labeled $-$ and $E$, to the root $E$ of the initial tree. The result is the second tree.

In the second step of the derivation $- E \implies  - ( E )$. Consequently, add three children, labeled $($, $E$, and $)$, to the leaf labeled $E$ of the second tree, to obtain the third tree with yield $- (E)$. Continuing in this fashion we obtain the complete parse tree as the sixth tree. □

![](images/Figure4.4.png)

Figure 4.4: Sequence of parse trees for derivation (4.8)

Since a parse tree ignores variations in the order in which symbols in sentential forms are replaced, there is a many-to-one relationship between derivations and parse trees. For example, both derivations (4.8) and (4.9), are associated with the same final parse tree of Fig. 4.4.

In what follows, we shall frequently parse by producing a leftmost or a rightmost derivation, since there is a one-to-one relationship between parse trees and either leftmost or rightmost derivations. Both leftmost and rightmost derivations pick a particular order for replacing symbols in sentential forms, so they too alter out variations in the order. It is not hard to show that every parse tree has associated with it a unique leftmost and a unique rightmost derivation.

### 4.2.5 Ambiguity

From Section 2.2.4, a grammar that produces more than one parse tree for some sentence is said to be ambiguous. Put another way, an ambiguous grammar is one that produces more than one leftmost derivation or more than one rightmost derivation for the same sentence.

**Example 4.11:** The arithmetic expression grammar (4.3) permits two distinct leftmost derivations for the sentence $\textbf{id} + \textbf{id} * \textbf{id}$:
$$
\begin{array}{ll}
&E &\implies & E + E        & E & \implies & E * E       \\
&  &\implies & \textbf{id} + E       &   & \implies & E + E * E   \\
&  &\implies & \textbf{id} + E * E   &   & \implies & \textbf{id} + E * E  \\
&  &\implies & \textbf{id} + \textbf{id} * E  &   & \implies & \textbf{id} + \textbf{id} * E \\
&  &\implies & \textbf{id} + \textbf{id} * \textbf{id} &   & \implies & \textbf{id} + \textbf{id} * \textbf{id} \\
\end{array}
$$

The corresponding parse trees appear in Fig. 4.5.

Note that the parse tree of Fig. 4.5(a) reflects the commonly assumed precedence of `+` and  `*`, while the tree of Fig. 4.5(b) does not. That is, it is customary to treat operator `*` as having higher precedence than `+`, corresponding to the fact that we would normally evaluate an expression like `a + b * c` as `a + (b * c)`, rather than as `(a + b) * c`. □

  ![](images/Figure4.5.png)

Figure 4.5: Two parse trees for $\textbf{id} + \textbf{id} * \textbf{id}$

For most parsers, it is desirable that the grammar be made unambiguous, for if it is not, we cannot uniquely determine which parse tree to select for a sentence. In other cases, it is convenient to use carefully chosen ambiguous grammars, together with disambiguating rules that “throw away” undesirable parse trees, leaving only one tree for each sentence.

### 4.2.6 Verifying the Language Generated by a Grammar

### 4.2.7 Context-Free Grammars Versus Regular Expressions


## 4.4 Top-Down Parsing

### 4.4.2 FIRST and FOLLOW

The construction of both top-down and bottom-up parsers is aided by two functions, FIRST and FOLLOW, associated with a grammar *G*. During top-down parsing, FIRST and FOLLOW allow us to choose which production to apply, based on the next input symbol. During panic-mode error recovery, sets of tokens produced by FOLLOW can be used as synchronizing tokens.

Define *FIRST($\alpha$)*, where $\alpha$ is any string of grammar symbols, to be the set of terminals that begin strings derived from $\alpha$. If $\alpha\overset*\implies\epsilon$, then $\epsilon$ is also in FIRST($\alpha$). For example, in Fig. 4.15, $A \overset * \implies c \gamma$, so $c$ is in FIRST($A$).

![](images/figure4.15.png)

Figure 4.15: Terminal $c$ is in FIRST($A$) and $a$ is in FOLLOW($A$)

For a preview of how FIRST can be used during predictive parsing, consider two $A$-productions $A \to \alpha|\beta$, where FIRST($\alpha$) and FIRST($\beta$) are disjoint sets. We can then choose between these $A$-productions by looking at the next input symbol $a$, since $a$ can be in at most one of FIRST($\alpha$) and FIRST($\beta$), not both. For instance, if $a$ is in FIRST($\beta$) choose the production $A \to \beta$. This idea will be explored when LL(1) grammars are defined in Section 4.4.3.

Define *FOLLOW(A)*, for nonterminal $A$, to be the set of terminals $a$ that can appear immediately to the right of $A$ in some sentential form; that is, the set of terminals $a$ such that there exists a derivation of the form $S \overset*\implies \alpha A a \beta$, for some $\alpha$ and $\beta$, as in Fig. 4.15. Note that there may have been symbols between $A$ and $a$, at some time during the derivation, but if so, they derived $\epsilon$ and disappeared. In addition, if $A$ can be the rightmost symbol in some sentential form, then \$ is in FOLLOW($A$); recall that \$ is a special "endmarker" symbol that is assumed not to be a symbol of any grammar.

To compute FIRST($X$) for all grammar symbols $X$, apply the following rules until no more terminals or can be added to any FIRST set.

1. If $X$ is a terminal, then $FIRST(X) = \{X\}$.
2. If $X$ is a nonterminal and $X \to Y_1 Y_2 \dots  Y_k$ is a production for some $k\ge 1$, then place $a$ in FIRST($X$) if for some $i$, $a$ is in FIRST($Y_i$), and is in all of FIRST($Y_1$), $\dots$, FIRST($Y _ {i-1}$); that is, $Y_1 \dots  Y _ {i-1} \overset*\implies \epsilon$. If $\epsilon$ is in FIRST($Y_j$) for all $j = 1, 2, \dots  k$, then add $\epsilon$ to FIRST($X$). For example, everything in FIRST($Y_1$) is surely in FIRST($X$). If $Y_1$ does not derive $\epsilon$, then we add nothing more to FIRST($X$), but if $Y_1 \overset*\implies \epsilon$, then we add FIRST($Y_2$), and so on.

3. If $X \to \epsilon$ is a production, then add $\epsilon$ to $FIRST(X)$.

Now, we can compute FIRST for any string $X_1 X_2 \dots  X_n$ as follows. Add to FIRST($X_1 X_2 \dots  X_n$) all non-$\epsilon$ symbols of FIRST($X_1$). Also add the non-$\epsilon$ symbols of FIRST($X_2$), if $\epsilon$ is in FIRST($X_1$); the non-$\epsilon$ symbols of FIRST($X_3$), if $\epsilon$ is in FIRST($X_1$) and FIRST($X_2$); and so on. Finally, add $\epsilon$ to FIRST($X_1 X_2 \dots  X_n$) if, for all $i$, $\epsilon$ is in FIRST($X_i$).

To compute FOLLOW($A$) for all nonterminals $A$, apply the following rules until nothing can be added to any FOLLOW set.

1.  Place \$ in FOLLOW($S$), where $S$ is the start symbol, and $ is the input right endmarker.

2.  If there is a production $A \to \alpha B \beta$, then everything in FIRST($\beta$) except $\epsilon$ is in FOLLOW($B$).

3.  If there is a production $A \to \alpha B$, or a production $A \to \alpha B \beta$, where FIRST($\beta$) contains $\epsilon$, then everything in FOLLOW($A$) is in FOLLOW($B$).

**Example 4.30:** Consider again the non-left-recursive grammar (4.28). Then:

1.  FIRST($F$) = FIRST($T$) = FIRST($E$) = {$($, $\textbf{id}$}. To see why, note that the two productions for $F$ have bodies that start with these two terminal symbols, **id** and the left parenthesis. $T$ has only one production, and its body starts with $F$. Since $F$ does not derive $\epsilon$, FIRST($T$) must be the same as FIRST($F$). The same argument covers FIRST($E$).

2.  FIRST($E'$) = {$+$, $\epsilon$}. The reason is that one of the two productions for $E'$ has a body that begins with terminal $+$, and the other's body is $\epsilon$. Whenever a nonterminal derives $\epsilon$, we place in FIRST for that nonterminal.

3.  FIRST($T'$) = {$ * $, $\epsilon$}. The reasoning is analogous to that for FIRST($E'$).

4.  FOLLOW($E$) = FOLLOW($E'$) = $\{), \$\}$. Since $E$ is the start symbol, FOLLOW($E$) must contain \$. The production body $(E)$ explains why the right parenthesis is in FOLLOW($E$). For $E'$, note that this nonterminal appears only at the ends of bodies of $E$-productions. Thus, FOLLOW($E'$) must be the same as FOLLOW($E$).

5.  FOLLOW($T$) = FOLLOW($T'$) = {$+$, $)$, \$}. Notice that $T$ appears in bodies only followed by $E'$. Thus, everything except $\epsilon$ that is in FIRST($E'$) must be in FOLLOW($T$); that explains the symbol $+$. However, since FIRST($E'$) contains $\epsilon$ (i.e., $E' \overset*\implies \epsilon$), and $E'$ is the entire string following $T$ in the bodies of the $E$-productions, everything in FOLLOW($E$) must also be in FOLLOW($T$). That explains the symbols \$ and the right parenthesis. As for $T'$, since it appears only at the ends of the $T$-productions, it must be that FOLLOW($T'$) = FOLLOW($T$).

6.  FOLLOW($F$) = {$+$,$ * $, $)$, \$}. The reasoning is analogous to that for $T$ in point (5).

□

## 4.5 Bottom-Up Parsing

A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root (the top). It is convenient to describe parsing as the process of building parse trees, although a front end may in fact carry out a translation directly without building an explicit tree. The sequence of tree snapshots in Fig. 4.25 illustrates a bottom-up parse of the token stream `id * id`, with respect to the expression grammar (4.1).

![](images/figure4.25.png)

Figure 4.25: A bottom-up parse for $\textbf{id} * \textbf{id}$

This section introduces a general style of bottom-up parsing known as shift reduce parsing. The largest class of grammars for which shift-reduce parsers can be built, the LR grammars, will be discussed in Sections 4.6 and 4.7. Although it is too much work to build an LR parser by hand, tools called automatic parser generators make it easy to construct efficient LR parsers from suitable grammars. The concepts in this section are helpful for writing suitable grammars to make effective use of an LR parser generator. Algorithms for implementing parser generators appear in Section 4.7.

### 4.5.1 Reductions

We can think of bottom-up parsing as the process of "reducing" a string $w$  to the start symbol of the grammar. At each reduction step, a specific substring matching the body of a production is replaced by the nonterminal at the head of that production.

The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds.

**Example 4.37:** The snapshots in Fig. 4.25 illustrate a sequence of reductions; the grammar is the expression grammar (4.1). The reductions will be discussed in terms of the sequence of strings

$$
\textbf{id} * \textbf{id},\ F * \textbf{id},\ T * \textbf{id},\ T * F,\ T,\ E
$$

The strings in this sequence are formed from the roots of all the subtrees in the snapshots. The sequence starts with the input string $\textbf{id} * \textbf{id}$. The first reduction produces $F * \textbf{id}$ by reducing the leftmost **id** to $F$, using the production $F \to \textbf{id}$. The second reduction produces $T * \textbf{id}$ by reducing $F$ to $T$.

Now, we have a choice between reducing the string $T$, which is the body of $E \to T$, and the string consisting of the second **id**, which is the body of $F \to \textbf{id}$. Rather than reduce $T$ to $E$, the second **id** is reduced to $T$, resulting in the string $T * F$. This string then reduces to $T$. The parse completes with the reduction of $T$ to the start symbol $E$. □

By definition, a reduction is the reverse of a step in a derivation (recall that in a derivation, a nonterminal in a sentential form is replaced by the body of one of its productions). The goal of bottom-up parsing is therefore to construct a derivation in reverse. The following derivation corresponds to the parse in Fig. 4.25:

$$
E \implies T \implies T * F \implies T * \textbf{id} \implies F * \textbf{id} \implies \textbf{id} * \textbf{id}
$$

This derivation is in fact a rightmost derivation.

### 4.5.2 Handle Pruning

Bottom-up parsing during a left-to-right scan of the input constructs a rightmost derivation in reverse. Informally, a "handle" is a substring that matches the body of a production, and whose reduction represents one step along the reverse of a rightmost derivation.

For example, adding subscripts to the tokens `id` for clarity, the handles during the parse of `id1 * id2` according to the expression grammar (4.1) are as in Fig. 4.26. Although `T` is the body of the production `E -> T`, the symbol `T` is not a handle in the sentential form `T * id2`. If `T` were indeed replaced by `E`, we would get the string `E * id2`, which cannot be derived from the start symbol `E`. Thus, the leftmost substring that matches the body of some production need not be a handle.

```
RIGHT SENTENTIAL FORM | HANDLE |REDUCING PRODUCTION
id1 * id2             | id1    | F -> id
F * id2               | F      | T -> F 
T* id2                | id2    | F -> id
T * F                 | T * F  | E -> T * F
```

Figure 4.26: Handles during a parse of $\textbf{id}_1 * \textbf{id}_2$

Formally, if $S \underset{rm}{\overset * \implies} \alpha Aw  \underset{rm}\implies  \alpha\beta w$, as in Fig. 4.27, then production $A \to \beta$  in the position following $\alpha$ is a handle of $\alpha\beta w$. Alternatively, a handle of a right-sentential form $\gamma$ is a production $A \to \beta$ and a position of $\gamma$ where the string $\beta$  may be found, such that replacing $\beta$ at that position by $A$ produces the previous right-sentential form in a rightmost derivation of $\gamma$.

![](images/figure4.27.png)

Figure 4.27: A handle $A \to \beta$ in the parse tree for $\alpha\beta w$

Notice that the string $w$ to the right of the handle must contain only terminal symbols. For convenience, we refer to the body $\beta$ rather than $A \to \beta$ as a handle. Note we say "a handle" rather than "the handle", because the grammar could be ambiguous, with more than one rightmost derivation of $\alpha\beta w$. If a grammar is unambiguous, then every right-sentential form of the grammar has exactly one handle.

A rightmost derivation in reverse can be obtained by "handle pruning". That is, we start with a string of terminals $w$ to be parsed. If $w$  is a sentence of the grammar at hand, then let $w  = \gamma_n$, where $\gamma_n$ is the nth right-sentential form of some as yet unknown rightmost derivation

$$
S = \gamma_0 \underset{rm}\implies \gamma_1 \underset{rm}\implies  \gamma_2 \underset{rm}\implies\dots\underset{rm}\implies \gamma_n = w
$$

To reconstruct this derivation in reverse order, we locate the handle $\beta_n$ in $\gamma_n$ and replace $\beta_n$ by the head of the relevant production $A_n \to \beta_n$ to obtain the previous right-sentential form $\gamma_{n-1}$. Note that we do not yet know how handles are to be found, but we shall see methods of doing so shortly.

We then repeat this process. That is, we locate the handle $\beta_{n-1}$ in $\gamma_{n-1}$ and reduce this handle to obtain the right-sentential form $\gamma_{n-2}$. If by continuing this process we produce a right-sentential form consisting only of the start symbol $S$, then we halt and announce successful completion of parsing. The reverse of the sequence of productions used in the reductions is a rightmost derivation for the input string.

### 4.5.3 Shift-Reduce Parsing

Shift-reduce parsing is a form of bottom-up parsing in which a stack holds grammar symbols and an input buffer holds the rest of the string to be parsed. As we shall see, the handle always appears at the top of the stack just before it is identified as the handle.

We use `$` to mark the bottom of the stack and also the right end of the input. Conventionally, when discussing bottom-up parsing, we show the top of the stack on the right, rather than on the left as we did for top-down parsing. Initially, the stack is empty, and the string $w$  is on the input, as follows:

```
STACK   INPUT
$          w$
```

During a left-to-right scan of the input string, the parser shifts zero or more input symbols onto the stack, until it is ready to reduce a string $\beta$ of grammar symbols on top of the stack. It then reduces $\beta$ to the head of the appropriate production. The parser repeats this cycle until it has detected an error or until the stack contains the start symbol and the input is empty:

```
STACK   INPUT
$S          $
```

Upon entering this configuration, the parser halts and announces successful completion of parsing. Figure 4.28 steps through the actions a shift-reduce parser might take in parsing the input string $\textbf{id}_1 * \textbf{id}_2$ according to the expression grammar (4.1).

```
STACK                INPUT  ACTION
------------+-------------+----------------------
$              id1 * id2 $  shift
$ id1              * id2 $  reduce by F -> id
$ F                * id2 $  reduce by T -> F
$ T                * id2 $  shift
$ T *                id2 $  shift
$ T * id2                $  reduce by F -> id
$ T * F                  $  reduce by T -> T * F
$ T                      $  reduce by E -> T
$ E                      $  accept
```

Figure 4.28: Configurations of a shift-reduce parser on input $\textbf{id}_1 * \textbf{id}_2$

While the primary operations are shift and reduce, there are actually four possible actions a shift-reduce parser can make: (1) shift, (2) reduce, (3) accept, and (4) error.

1. *Shift*. Shift the next input symbol onto the top of the stack.

2. *Reduce*. The right end of the string to be reduced must be at the top of the stack. Locate the left end of the string within the stack and decide with what nonterminal to replace the string.

3. *Accept*. Announce successful completion of parsing.

4. *Error*. Discover a syntax error and call an error recovery routine.

The use of a stack in shift-reduce parsing is justified by an important fact: the handle will always eventually appear on top of the stack, never inside. This fact can be shown by considering the possible forms of two successive steps in any rightmost derivation. Figure 4.29 illustrates the two possible cases. In case (1), $A$ is replaced by $\beta B y$, and then the rightmost nonterminal $B$ in the body $\beta B y$ is replaced by $\gamma$. In case (2), $A$ is again expanded first, but this time the body is a string $y$ of terminals only. The next rightmost nonterminal $B$ will be somewhere to the left of $y$.

![](images/figure4.29.png)

Figure 4.29: Cases for two successive steps of a rightmost derivation

In other words:

$$
\begin{array}{l}
S \underset{rm}{\overset*\implies}\alpha A z \underset{rm}\implies  \alpha\beta B y z \underset{rm}\implies  \alpha\beta\gamma y z \\
S \underset{rm}{\overset*\implies}\alpha B x A z \underset{rm}\implies\alpha B x y z \underset{rm}\implies \alpha\gamma x y z
\end{array}
$$

Consider case (1) in reverse, where a shift-reduce parser has just reached the configuration

```
STACK        INPUT
$ α β γ      y z $
```

The parser reduces the handle $\gamma$ to $B$ to reach the configuration

```
$ α β B      y z $
```

The parser can now shift the string $y$ onto the stack by a sequence of zero or more shift moves to reach the configuration

```
$ α β B y      z $
```

with the handle $\beta B y$ on top of the stack, and it gets reduced to $A$.

Now consider case (2). In configuration

```
$ α γ      x y z $
```

the handle $\gamma$  is on top of the stack. After reducing the handle $\gamma$ to $B$, the parser can shift the string $xy$ to get the next handle $y$ on top of the stack, ready to be reduced to $A$.

```
$ α B x y      z $
```

In both cases, after making a reduction the parser had to shift zero or more symbols to get the next handle onto the stack. It never had to go into the stack to find the handle.

### 4.5.4 Conflicts During Shift-Reduce Parsing

There are context-free grammars for which shift-reduce parsing cannot be used. Every shift-reduce parser for such a grammar can reach a configuration in which the parser, knowing the entire stack contents and the next input symbol, cannot decide whether to shift or to reduce (a *shift/reduce conflict*), or cannot decide which of several reductions to make (a *reduce/reduce conflict*). We now give some examples of syntactic constructs that give rise to such grammars. Technically, these grammars are not in the LR(k) class of grammars defined in Section 4.7; we refer to them as non-LR grammars. The $k$ in LR(k) refers to the number of symbols of lookahead on the input. Grammars used in compiling usually fall in the LR(1) class, with one symbol of lookahead at most.

**Example 4.38:** An ambiguous grammar can never be LR. For example, consider the dangling-else grammar (4.14) of Section 4.3:

$$
\begin{array}{lrl}
stmt &\to& \textbf{if } expr \textbf{ then }stmt \\
&|& \textbf{if }expr\textbf{ then }stmt\textbf{ else }stmt \\
&|& other \\
\end{array}
$$

If we have a shift-reduce parser in configuration
$$
\begin{array}{lr}
\text{STACK}&\text{INPUT}\\
\dots\textbf{if}\ expr\ \textbf{then}\ stmt& \textbf{else}\dots \$
\end{array}
$$

we cannot tell whether $\textbf{if } expr \textbf{ then }stmt$ is the handle, no matter what appears below it on the stack. Here there is a shift/reduce conflict. Depending on what follows the **else** on the input, it might be correct to reduce $\textbf{if } expr \textbf{ then }stmt$ to $stmt$, or it might be correct to shift **else** and then to look for another $stmt$ to complete the alternative $\textbf{if }expr\textbf{ then }stmt\textbf{ else }stmt$.

Note that shift-reduce parsing can be adapted to parse certain ambiguous grammars, such as the if-then-else grammar above. If we resolve the shift/reduce conflict on **else** in favor of shifting, the parser will behave as we expect, associating each **else** with the previous unmatched **then**. We discuss parsers for such ambiguous grammars in Section 4.8.□

Another common setting for conflicts occurs when we know we have a handle, but the stack contents and the next input symbol are insufficient to determine which production should be used in a reduction. The next example illustrates this situation.

**Example 4.39:** Suppose we have a lexical analyzer that returns the token name **id** for all names, regardless of their type. Suppose also that our language invokes procedures by giving their names, with parameters surrounded by parentheses, and that arrays are referenced by the same syntax. Since the translation of indices in array references and parameters in procedure calls are different, we want to use different productions to generate lists of actual parameters and indices. Our grammar might therefore have (among others) productions such as those in Fig. 4.30.

$$
\begin{array}{lrcl}
(1)&   stmt              &\to&   \textbf{id}\ (\ parameter\_list\ ) \\
(2)&   stmt              &\to&   expr\ :=\ expr \\
(3)&   parameter\_list   &\to&   parameter\_list\ ,\ parameter \\
(4)&   parameter\_list   &\to&   parameter \\
(5)&   parameter         &\to&   \textbf{id} \\
(6)&   expr              &\to&   \textbf{id}\ (\ expr\_list\ ) \\
(7)&   expr              &\to&   \textbf{id} \\
(8)&   expr\_list        &\to&   expr\_list\ ,\ expr \\
(9)&   expr\_list        &\to&   expr
\end{array}
$$

Figure 4.30: Productions involving procedure calls and array references

A statement beginning with `p(i, j)` would appear as the token stream $\textbf{id}\  (\ \textbf{id}\ , \textbf{id}\ )$ to the parser. After shifting the first three tokens onto the stack, a shift-reduce parser would be in configuration

$$
\begin{array}{lr}
\text{STACK}&\text{INPUT}\\
\dots  \textbf{id}\ (\ \textbf{id}&   ,\ \textbf{id}\ ) \dots
\end{array}
$$

It is evident that the **id** on top of the stack must be reduced, but by which production? The correct choice is production (5) if `p` is a procedure, but production (7) if `p` is an array. The stack does not tell which; information in the symbol table obtained from the declaration of `p` must be used.

One solution is to change the token **id** in production (1) to **procid** and to use a more sophisticated lexical analyzer that returns the token name **procid** when it recognizes a lexeme that is the name of a procedure. Doing so would require the lexical analyzer to consult the symbol table before returning a token.

If we made this modification, then on processing `p(i, j)` the parser would be either in the configuration

$$
\begin{array}{lr}
\text{STACK}&\text{INPUT}\\
\dots  \textbf{procid}\ (\ \textbf{id}&   ,\ \textbf{id}\ ) \dots
\end{array}
$$

or in the configuration above. In the former case, we choose reduction by production (5); in the latter case by production (7). Notice how the third symbol from the top of the stack determines the reduction to be made, even though it is not involved in the reduction. Shift-reduce parsing can utilize information far down in the stack to guide the parse.

□

